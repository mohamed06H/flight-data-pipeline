# Option 2 - Streaming pipeline with Amazon Athena to query results
## Description
What does your solution do? Which tech stack have you used?

The project ingests data from Sky-scrapper API in streaming to find lists of flights in a specific route (Example London -> New York City on September 10th) 

Tech Stack:
- **Source API:** [Air Scraper API](https://rapidapi.com/apiheya/api/sky-scrapper) (Free Plan 500 requests / month)
- **Data Producer:** Python Script running as a cron job every minute on an EC2 instance
- **Message broker:** Amazon MSK
- **Data Consumer:** Python Script continuously consuming kafka topic, running on an EC2 instance
- **Persistence layer:** S3 Bucket where data is partitioned by year, month, day
- **Visualization layer:** AWS Glue and AWS Athena 
- **Deployment Automation:** IaC with Terraform

## Prerequisites

- [Terraform](https://www.terraform.io/downloads.html)
- [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html)
- [AWS Account](https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/)

## Requirements
Do you have a `requirements.txt` file? Or maybe you've used `Poetry` for your dependencies? Either case, please provide with instructions on how to install the required libraries.

```bash
Any command to be typed from my CLI?
```
## Deploying the infrastructure

Locate yourself under the `terraform` directory and run the following commands:

```bash
aws configure  # enter your AWS credentials

terraform init
terraform plan  # this is optional, it will show you what will be deployed - check that 23 resources will be created
terraform apply
```

It will take around `25 minutes` to complete the deployment. You will see the following output:

```bash
Apply complete! Resources: 34 added, 0 changed, 0 destroyed.

Outputs:

execute_this_to_access_the_bastion_host = "ssh ec2-user@x.xxx.xxx.xxx -i cert.pem"
execute_this_to_access_the_ec2_data_producer = "ssh ec2-user@x.xxx.xxx.xxx -i cert.pem"
execute_this_to_access_the_kafka_consumer = "ssh ec2-user@x.xxx.xxx.xxx -i cert.pem"
```

You probably have noticed as well a new file under your terraform folder, `cert.pem`. This is the private key that you need
to use to access the bastion host, data producer and kafka consumer instances. You can use the command provided in the output to access the instances.


## Testing the Kafka Cluster

Connect to the bastion host and run the following commands

```bash
# Check your bootstrap servers
more bootstrap-servers

# Check the correct creation of your topic: you should see flight-kafka-topic (defined in variables.tf)
kafka-topics.sh --list --bootstrap-server $(cat bootstrap-servers)

# Consume messages through console: you should see a new message everytime the producer runs the cron job (every minute)
kafka-console-consumer.sh  --bootstrap-server $(cat bootstrap-servers) --topic flight-kafka-topic

Outputs: 

[ec2-user@ip-10-0-4-252 ~]$ kafka-topics.sh --list --bootstrap-server $(cat bootstrap-servers)
__amazon_msk_canary
__consumer_offsets
flight-kafka-topic
[ec2-user@ip-10-0-4-252 ~]$ kafka-console-consumer.sh  --bootstrap-server $(cat bootstrap-servers) --topic flight-kafka-topic
{"message":"You have exceeded the MONTHLY quota for Requests on your current plan, BASIC. Upgrade your plan at https:\/\/rapidapi.com\/apiheya\/api\/sky-scrapper"}
{"message":"You have exceeded the MONTHLY quota for Requests on your current plan, BASIC. Upgrade your plan at https:\/\/rapidapi.com\/apiheya\/api\/sky-scrapper"}

```
## Testing the Data Producer

Connect to the data producer instance wait the first scheduled job to run:

```bash
# Check the data producer log file generated by the python script in <CLONE_DIR>/code/producer/data_producer.py
tail -f data_producer.log

Outputs:

[ec2-user@ip-10-0-5-74 ~]$ ls
data_producer.log  flight-data-pipeline
[ec2-user@ip-10-0-5-74 ~]$ tail -f data_producer.log 
-- Data is read successfully from API 2024-09-01 11:36:01.521448
--- Sent message to Kafka topic 'flight-kafka-topic'
-- Data is read successfully from API 2024-09-01 11:37:02.107995
--- Sent message to Kafka topic 'flight-kafka-topic'

# Temporarily stop the ingestion by stopping crond service
sudo service crond stop

# Start the crond service again
sudo service crond start
```

## Testing the Kafka Consumer

Connect to the kafka consumer instance wait for the github repo to be pulled and run the following command:

```bash
# Check kafka consumer log file
tail -f kafka_consumer.log
```

Debug the consumer:
```bash
# Find the process running the consumer script 
ps aux | grep kafka_to_s3_consumer.py
# Kill the process
sudo kill <PID>
# Start the consumer again
nohup python3 $CLONE_DIR/code/consumer/kafka_to_s3_consumer.py $BOOTSTRAP_SERVERS $SECURITY_PROTOCOL $TOPIC_NAME  $S3_BUCKET_NAME >> /home/ec2-user/kafka_consumer.log 2>&1 &
```

Check the s3 output bucket on your account console:
![img.png](screenshots/s3_json_data_bucket.png)

## Destroying the infrastructure

Once you are done with the testing, you can destroy the infrastructure by running the following command:

```bash
terraform destroy
```

## Design
Provide a diagram of your solution, it can be the same one as in the pdf or one you've done yourself. Explain the diagram.

## Developer Guide
If I was to continue as a developer with the work you've just done, which things are essential for me to know in order to be able to do so?
Can you provide with step-by-step assistance of what you've done so far with detailed screenshots? If something is not working as it should, this a good place to state that as well.


# Author
Provide your contact details.
